# 11 - Join Algorithms

![1.jpg](assets/1679994712588-1505c704-8da3-4984-acba-e1909795e3e2.jpeg)

![2.jpg](assets/1679994710692-8683d649-a89e-45fc-9bee-67784dbac22b.jpeg)

![3.jpg](assets/1679994710350-8e436f66-ad9d-41a5-8338-ba7152cd9286.jpeg)

# Joins

The goal of a good database design is to minimize the amount of information repetition. This is why tables are composed based on normalization theory. Joins are therefore needed to reconstruct the original tables.
This class will cover **inner** **equijoin** algorithms for combining two-tables. An *equijoin* algorithm joins tables where keys are equal. These algorithms can be tweaked to support other joins.

![4.jpg](assets/1679994711391-eaa8fe80-3e1c-4c61-9fb2-6f2deea481ad.jpeg)

![5.jpg](assets/1679994711401-3f8a9d42-f7cd-4d9e-8843-9f499504c37f.jpeg)

## Operator Output

For a tuple $r ∈ R$ and a tuple $s ∈ S$ that match on join attributes, the join operator concatenates r and s together into a new output tuple.
In reality, contents of output tuples generated by a join operator varies. It depends on the DBMS’s query processing model, storage model, and the query itself. There are multiple approaches to the contents of the join operator output.
• **Data:**  This approach copies the values for the attributes in the outer and inner tables into tuples put into an intermediate result table just for that operator. The advantage of this approach is that future operators in the query plan never need to go back to the base tables to get more data. The disadvantage is that this requires more memory to materialize the entire tuple. This is called ***early materialization***. The DBMS can also do additional computation and omit attributes which will not be needed later in the query to further optimize this approach.
• **Record Ids**: In this approach, the DBMS only copies the join keys along with the record ids of the matching tuples. This approach is ideal for column stores because the DBMS does not copy data that is not needed for the query. This is called_ **late materialization**_.

![6.jpg](assets/1679994719122-388a2a11-23c1-490a-8925-e32468603e5c.jpeg)

![7.jpg](assets/1679994719848-575920d3-804c-48b5-8d5d-7112fd45e7c3.jpeg)

![8.jpg](assets/1679994720668-b90fee0e-2b09-4a77-a160-15431344e0d4.jpeg)

![9.jpg](assets/1679994721116-52828864-5f39-43aa-a53c-3dab19ee1aa8.jpeg)

![10.jpg](assets/1679994721483-35120685-dc97-4c8f-a536-a4c3de3767a7.jpeg)

![11.jpg](assets/1679994724674-4b330db9-9f29-4445-9237-3121c71f1415.jpeg)

![12.jpg](assets/1679994726462-907fb638-d89b-413e-8cd3-ce7e5d71d86f.jpeg)

![13.jpg](assets/1679994726555-b99981e4-7587-4712-affd-cfe1d5b7cdcf.jpeg)

![14.jpg](assets/1679994727722-024772b6-1b42-44b4-b8aa-d7edfa954648.jpeg)

## Cost Analysis

The cost metric used here to analyze the different join algorithms will be the number of disk I/Os used to compute the join. This includes I/Os incurred by reading data from disk as well as writing any intermediate data out to disk.
Note that only I/Os from computing the join are considered, while I/O incurred when outputting the result is not. This is because the output cost depends on the data, moreover, the output for any join algorithm will be the same and therefore the cost will not change among the different algorithms.

Variables used in this lecture:
• $M$ pages in table $R$ (Outer Table), $m$ tuples total
• $N$ pages in table $S$ (Inner Table), $n$ tuples total

![15.jpg](assets/1679994727586-dfab5315-311f-4160-9107-ed5c7f0e6825.jpeg)

In general, there will be many algorithms/optimizations which can reduce join costs in some cases, but no single algorithm which works well in every scenario.

![16.jpg](assets/1679994729080-99911764-cb1f-4325-a7c1-fd92215acfa2.jpeg)

![17.jpg](assets/1679994731069-1b43a2c5-60aa-4038-a0ea-fbb1ecead6cd.jpeg)

# Nested Loop Join

At a high-level, this type of join algorithm is comprised of two nested **for** loops that iterate over the tuples in both tables and compares each of them pairwise. If the tuples match the join predicate, then output them. The table in the outer **for** loop is called the *outer table*, while the table in the inner **for** loop is called the *inner table.*

![18.jpg](assets/1679994734931-dc13c43d-4283-410c-ae04-c049cabb4bbe.jpeg)

The DBMS will always want to use the “smaller” table as the outer table. Smaller can be in terms of the number of tuples or the number of pages. The DBMS will also want to buffer as much of the outer table in memory as possible. It can also try to leverage an index to find matches in the inner table.

## Simple Nested Loop Join

For each tuple in the outer table, compare it with each tuple in the inner table. This is the worst case scenario where the DBMS must do an entire scan of the inner table for each tuple in the outer table without any caching or access locality.
**Cost**: $M + (m × N)$

![19.jpg](assets/1679994735410-77dbd117-55ba-4ba8-8052-b295732090a5.jpeg)

![20.jpg](assets/1679994735386-3dac945e-55a8-43f7-be74-ea29836e9a62.jpeg)

![21.jpg](assets/1679994736535-c8e8d19d-c331-41c3-a8d9-cfc659b16c01.jpeg)

## Block Nested Loop Join

For each block in the outer table, fetch each block from the inner table and compare all the tuples in those two blocks. This algorithm performs fewer disk accesses because the DBMS scans the inner table for every outer table block instead of for every tuple.
**Cost**: $M + (M × N)$

![22.jpg](assets/1679994738131-cf120493-6353-431e-a3db-0daf034f3bff.jpeg)

![23.jpg](assets/1679994742379-97490d45-23b9-4c83-b33f-57169bd8115d.jpeg)

![24.jpg](assets/1679994741609-28112a7d-0482-4bb2-9bff-090394406a58.jpeg)

If the DBMS has $B$ buffers available to compute the join, then it can use $B − 2$ buffers to scan the outer table. It will use one buffer to scan the inner table and one buffer to store the output of the join.
**Cost**: $M + (\lceil \frac{M}{B−2} \rceil × N)$

![25.jpg](assets/1679994743946-5c56dcfd-1f10-4e9f-af98-ec49d930b511.jpeg)

![26.jpg](assets/1679994745228-004d03c4-5a9b-4aed-8153-503a3ac552a3.jpeg)

![27.jpg](assets/1679994744512-01874829-72d1-4aa8-a63f-a31200e8b3f5.jpeg)

![28.jpg](assets/1679994747415-e34dc943-f1de-470e-b14a-af171971cf0b.jpeg)

## Index Nested Loop Join

The previous nested loop join algorithms perform poorly because the DBMS has to do a sequential scan to check for a match in the inner table. However, if the database already has an index for one of the tables on the join key, it can use that to speed up the comparison. The DBMS can either use an existing index or build a temporary one for the join operation.
The outer table will be the one without an index. The inner table will be the one with the index.
Assume the cost of each index probe is some constant value C per tuple.
**Cost**: $M + (m × C)$

![29.jpg](assets/1679994749676-fbad277a-a96c-4740-93bb-b1e3a613027d.jpeg)

![30.jpg](assets/1679994751112-de60ce6d-0985-4cac-a9c7-306eba2d563f.jpeg)

![31.jpg](assets/1679994749791-471fd9f8-557e-438e-9299-f71136ab5433.jpeg)

# Sort-Merge Join

At a high-level, a sort-merge join sorts the two tables on their join key(s). The DBMS can use the external mergesort algorithm for this. It then steps through each of the tables with cursors and emits matches (like in mergesort).
This algorithm is useful if one or both tables are already sorted on join attribute(s) (like with a clustered index) **or if the output needs to be sorted on the join key anyways**.
The worst case scenario for this algorithm is if the join attribute for all the tuples in both tables contain the same value, which is very unlikely to happen in real databases. In this case, the cost of merging would be $M · N$. Most of the time though, the keys are mostly unique so the merge cost is approximately $M + N$.

![32.jpg](assets/1679994751127-9866929e-f527-4746-b7c2-bd7ea0c6e881.jpeg)

![33.jpg](assets/1679994753091-aeca239b-cffe-4e2a-9f5c-7215635bd971.jpeg)

![34.jpg](assets/1679994755177-83127cd4-6f18-4ef2-8b35-ab6afccaa046.jpeg)

![35.jpg](assets/1679994755316-a25d3007-257c-4827-ab3d-cfcf3e6437b7.jpeg)

![36.jpg](assets/1679994756554-acee94c9-9c3c-4860-ab1b-61a780c6b003.jpeg)

![37.jpg](assets/1679994756695-44ffc03a-234c-4a76-8472-7180e83a4ed9.jpeg)

![38.jpg](assets/1679994758712-ed099f8f-ac6d-40f8-b199-c2a5ef2094fa.jpeg)

![39.jpg](assets/1679994769006-a216b953-8e93-4aa0-82c5-fc20adc5bc95.jpeg)

![40.jpg](assets/1679994772962-93fa13e9-7f78-4960-ad96-e7a4161728ef.jpeg)

![41.jpg](1679994776464-793e37c7-f214-41bf-808b-2258b913bc42.jpeg)

![42.jpg](assets/1679994777655-870e7148-21d9-4d77-8709-be8a1b9b8235.jpeg)

![43.jpg](assets/1679994779332-0e208491-7333-45c8-9b3e-4bb90bc07a9d.jpeg)

![44.jpg](assets/1679994783263-6d6c5ed9-291b-465a-a6b0-e8f2ba046923.jpeg)

![45.jpg](assets/1679994785020-9c1580b8-7dc6-4ba7-897b-d685dc2c2bdf.jpeg)

![46.jpg](assets/1679994787602-acd03aeb-d36e-4ad7-bcc5-c19c71d48da3.jpeg)

![47.jpg](assets/1679994789700-f6045b12-26b2-479e-8505-117d0de0c202.jpeg)

Assume that the DBMS has B buffers to use for the algorithm:

- Sort Cost for Table $R$: $2M × ( 1 + \lceil log_{B−1} \lceil \frac{M}{B} \rceil \rceil)$
- Sort Cost for Table $S$: $2N × ( 1 + \lceil log_{B−1} \lceil \frac{N}{B} \rceil \rceil)$
- Merge Cost: $(M + N)$

**Total Cost: Sort + Merge**

![48.jpg](assets/1679994785147-896b90d3-f5e4-4917-8404-fc83b37ffbf7.jpeg)

![49.jpg](assets/1679994793275-4b59918b-91c5-4cd6-9c7e-35fc2794ef05.jpeg)

![50.jpg](assets/1679994793016-cb64647f-928b-457d-a744-a19870bd0e44.jpeg)

![51.jpg](assets/1679994793809-768f6bd1-3090-45a9-8a1f-06bbc4384cce.jpeg)

# Hash Join

The high-level idea of the hash join algorithm is to use a hash table to split up the tuples into smaller chunks based on their join attribute(s). This reduces the number of comparisons that the DBMS needs to perform per tuple to compute the join. Hash joins can only be used for equi-joins on the complete join key.

If tuple $r ∈ R$ and a tuple $s ∈ S$ satisfy the join condition, then they have the same value for the join attributes. If that value is hashed to some value $i$, the $R$ tuple has to be in bucket $r_i$ , and the $S$ tuple has to be in bucket $s_i$ . Thus, the $R$ tuples in bucket $r_i$ need only to be compared with the $S$ tuples in bucket $s_i$ .

![52.jpg](assets/1679994795467-608ff949-a934-46d4-ba31-120fb457c7ff.jpeg)

## Basic Hash Join

• **Phase #1**  **– Build**: First, scan the outer relation and populate a hash table using the hash function $h_1$ on the join attributes. The key in the hash table is the join attributes. The value depends on the implementation (can be full tuple values or a tuple id).
• **Phase #2**  **– Probe**: Scan the inner relation and use the hash function $h_1$ on each tuple’s join attributes to jump to the corresponding location in the hash table and find a matching tuple. Since there may be collisions in the hash table, the DBMS will need to examine the original values of the join attribute(s) to determine whether tuples are truly matching.

![53.jpg](assets/1679994798275-33b2f519-d459-4a38-835e-1770918a8a4e.jpeg)

![54.jpg](assets/1679994799156-01647f46-6dfc-42cd-aa97-2d9e38e107ff.jpeg)

![55.jpg](assets/1679994799885-f0d5b0dc-20ee-49e5-98db-b69b25b23ffa.jpeg)

If the DBMS knows the size of the outer table, the join can use a static hash table. If it does not know the size, then the join has to use a dynamic hash table or allow for overflow pages.

![56.jpg](assets/1679994799977-ff633b6f-42ee-43f7-b426-5742d45cb60b.jpeg)

A table with $N$ pages needs around $\sqrt{N}$ buffers. The above approach creates $B − 1$ spill partitions of size at most $B$ blocks in Phase #1,# so assuming that the hash function distributes records evenly, the largest table that can be hashed with this approach is $B · (B − 1)$ buffers. If the hash function is not uniform, a fudge factor $f > 1$ can be introduced, therefore the largest such table is $B · \sqrt{f · N}$.(注意这里的Hash表的每个blocks都放在disk中的)

![57.jpg](assets/1679994800380-6f5cb55b-1145-4d19-8e6c-6e028114f0ed.jpeg)

One optimization for the probe phase is the usage of a [Bloom Filter](https://en.wikipedia.org/wiki/Bloom_filter). This is a probabilistic data structure that can fit in CPU caches and answer the question is key $x$ in the hash table? with either definitely no or **probably** yes. This can reduce the amount of disk I/O by preventing disk reads that do not result in an emitted tuple.

![58.jpg](assets/1679994801487-55bc0b85-c3c6-45fc-9881-c93131c73d27.jpeg)

![59.jpg](assets/1679994802599-aa4727d9-1471-4483-b334-c6cdd52a3a55.jpeg)

![60.jpg](assets/1679994803969-d76ec0a2-a452-45d1-b320-2df6d59ebdc7.jpeg)

![61.jpg](assets/1679994803642-cd3d280c-df1d-4c99-9760-655a5089b6bc.jpeg)

![62.jpg](assets/1679994806061-63bef4d4-6dbc-4755-9c12-a7810874349c.jpeg)

![63.jpg](assets/1679994809829-7f4313ab-228c-43f5-a64b-89f1dafa2b5a.jpeg)

![64.jpg](assets/1679994811313-04dd636d-f41e-4d7b-8e57-d90b29d6499d.jpeg)

![65.jpg](assets/1679994812542-f16c1fd1-11c2-42c0-a6b1-aefab18e7daa.jpeg)

## Grace Hash Join / Partitioned Hash Join

When the tables do not fit on main memory, the DBMS has to swap tables in and out essentially at random, which leads to poor performance. The Grace Hash Join is an extension of the basic hash join that also hashes the inner table into partitions that are written out to disk.

![66.jpg](assets/1679994811502-22cf77db-b42a-447b-8cbe-f4fea83f170f.jpeg)

•** Phase #1# – Build**: First, scan both the outer and inner tables and populate a hash table using the hash function $h_1$ on the join attributes. The hash table’s buckets are written out to disk as needed. If a single bucket does not fit in memory, the DBMS can use *recursive partitioning* with different hash function $h_2$ (where $h_1 \neq h_2$ ) to **further divide the bucket**. This can continue recursively until the buckets fit  into memory.
• **Phase #2 – Probe:**  For each bucket level, retrieve the corresponding pages for both outer and inner tables. Then, perform a nested loop join on the tuples in those two pages. The pages will fit in memory, so this join operation will be fast.
Partitioning Phase Cost: $2 × (M + N)$
Probe Phase Cost: $(M + N)$
Total Cost: $3 × (M + N)$

![67.jpg](assets/1679994818544-4bdde9bc-a5c3-48df-a11e-e8e0e1e65c0e.jpeg)

![68.jpg](assets/1679994822690-dd0a5906-470f-44fc-b81d-f9a18e4b978b.jpeg)

![69.jpg](assets/1679994827280-491fee75-141f-4cc0-8029-03e29f1226b2.jpeg)

![70.jpg](assets/1679994827182-4ccc8f10-a72b-48e6-ac47-4d0134766589.jpeg)

![71.jpg](assets/1679994825123-17a2df07-343b-4677-be77-26134d4689ab.jpeg)

![72.jpg](assets/1679994827158-0b15eda1-98d6-4c3f-82e8-7bcc96430ed7.jpeg)

![73.jpg](assets/1679994828767-562aa2b0-0a8d-47c8-b919-84dc4323bfe4.jpeg)

![74.jpg](assets/1679994831401-ac48f96b-67fa-48f7-9fae-315fb6e3da77.jpeg)

![75.jpg](assets/1679994831836-e61e0f3b-b115-40c9-be40-e4ad15186b33.jpeg)

![76.jpg](assets/1679994831509-24cdb125-143e-485d-a5d7-f8dd2ac73282.jpeg)

![77.jpg](assets/1679994832450-aad20fc6-3cea-486a-adaa-738df6a9e47e.jpeg)

![78.jpg](assets/1679994832550-713b8f6f-b9df-4a03-ade9-07f36d7b1a9e.jpeg)

![79.jpg](assets/1679994835514-b641a0df-af91-44f1-b37a-0357658cfff2.jpeg)

![80.jpg](assets/1679994836753-e29d5f76-8578-4b03-9862-f57f45c4ed3a.jpeg)

![81.jpg](assets/1679994836755-9871894a-b217-408c-8e86-de0c6e2c4381.jpeg)

![82.jpg](assets/1679994837734-fa44a3ae-fbe4-4df9-8152-1ea5c99b09c2.jpeg)

Hybrid hash join optimization: adapts between basic hash join and Grace hash join; if the keys are skewed, keep the hot partition in-memory and immediately perform the comparison instead of spilling it to disk. Difficult to implement correctly.

![83.jpg](assets/1679994839319-231922f5-822f-4f8c-ae36-8bb318832c3b.jpeg)

![84.jpg](assets/1679994839929-f3617a16-7e9b-439c-acb9-8a9a37b63830.jpeg)

# Conclusion

Joins are an essential part of interacting with relational databases, and it is therefore critical to ensure that a DBMSs has efficient algorithms to execute joins.

![85.jpg](assets/1679994840926-519cce76-1e10-43e6-b511-2a91a43b347c.jpeg)

The table above assume the following: $M=1000$, $m= 500$, $n =100000$, $N=500$, $n= 40000$, $B = 100$ and 0.1 ms per I/O. Sort cost is $R + S = 4000 + 2000$ IOs, where $R = 2 · M · ( 1 + ⌈ log_{B−1} ⌈ M/B ⌉⌉ ) = 2000 · (1 + ⌈ log_{99} ⌈ 1000/100 ⌉ ) = 4000$ and $S = 2 · N · ( 1 + ⌈ log_{B−1} ⌈ N/B ⌉⌉ ) = 1000 · (1 + ⌈ log_{99} ⌈ 500/100 ⌉ ) = 2000$.

Hash joins are almost always better than sort-based join algorithms, but there are cases in which sortingbased joins would be preferred. This includes queries on non-uniform data, when the data is already sorted on the join key, and when the result needs to be sorted. Good DBMSs will use either, or both.

![86.jpg](assets/1679994840669-608086d1-4848-4f28-90b2-39b881ebaee1.jpeg)

![87.jpg](assets/1679994840660-4f483036-aea6-403d-b639-b7993910d78a.jpeg)

![88.jpg](assets/1679994841567-255a3ee5-4d3c-4177-a0fe-0ee91f22cd92.jpeg)

![89.jpg](assets/1679994842075-32846779-5bf2-4508-9f61-70fc5b1cc7d9.jpeg)

![90.jpg](assets/1679994843139-28ca2b26-5e67-4b6a-b0d2-fd89f6d133d9.jpeg)

![91.jpg](assets/1679994843396-d1681885-756a-43ee-88a8-4d6e889a0ac8.jpeg)

![92.jpg](assets/1679994843734-fdcc2446-da0c-444f-927d-58623b92cd3a.jpeg)

![93.jpg](assets/1679994844758-e2b6ef79-3043-4595-abd8-c75a6009ff5b.jpeg)

![94.jpg](assets/1679994844048-46e5cc54-226b-4b26-885e-a6e8eb481fbc.jpeg)
